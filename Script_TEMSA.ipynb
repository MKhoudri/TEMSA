{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veuillez entrer les répertoires des fichiers d'entrée et de sortie.\n",
    "#Pensez à adapter la regex pour remplacer les caractères spéciaux.(Sinon deux regex sont prévu pour les données FFB et YBH)\n",
    "#Notez bien que vous pouvez paramétrer le choix des tiers du fichier de sortie, \n",
    "#Il suffit de mettre en commentaire (#) les tiers dont vous voulez vous passer dans la dernière cellule.\n",
    "path_in = ('Répertoire des fichiers d entrées')\n",
    "path_out = ('Répertoire des fichiers de sortie')\n",
    "path_LeFFF = ('Répertoire du dictionnaire français LeFFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pympi\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from lang_trans.arabic import buckwalter\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.pos import FarasaPOSTagger as tagger\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Le chemin d’accès spécifié est introuvable: 'Répertoire des fichiers d entrées'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-7c400eee046d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlist_fichier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.eaf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mlist_fichier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlist_fichier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Le chemin d’accès spécifié est introuvable: 'Répertoire des fichiers d entrées'"
     ]
    }
   ],
   "source": [
    "list_fichier = []\n",
    "for filename in os.listdir(path_in):\n",
    "    if filename.endswith('.eaf'):\n",
    "        list_fichier.append(filename)\n",
    "list_fichier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_dict_bruit(list_fichier):\n",
    "    list_bruit = []\n",
    "    dic_bruit={}\n",
    "    for fichier in list_fichier:    \n",
    "\n",
    "        fichier = path_in+fichier\n",
    "        eafob = pympi.Elan.Eaf(fichier)\n",
    "\n",
    "        tier_names = eafob.get_tier_names()\n",
    "        for tier_name in tier_names:\n",
    "            spk = eafob.get_annotation_data_for_tier(tier_name)\n",
    "\n",
    "            def list_tuple (tuple_spk):\n",
    "                list_spk = []\n",
    "                for i in tuple_spk:\n",
    "                    x = [i[0], i[1], i[2]]\n",
    "                    list_spk.append(x)\n",
    "                return list_spk\n",
    "\n",
    "            list_spk = list_tuple(spk)\n",
    "\n",
    "            #Référencer  les caractères de la transcription (appliquer sur les fichiers dès le debut)\n",
    "            for i in list_spk:\n",
    "                p = re.compile(r'\\[(.*?)\\]')\n",
    "                c = re.compile(r'\\{(.*?)\\}')\n",
    "                list_bruit.append(''.join(p.findall(i[2])))\n",
    "                list_bruit.append(''.join(c.findall(i[2])))\n",
    "\n",
    "            for i in list_bruit:\n",
    "                x = i.split('/')\n",
    "                x = x[0].lstrip('=!')\n",
    "                x = '#'+x\n",
    "                dic_bruit[x] = '['+i+']'\n",
    "\n",
    "    return dic_bruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict_bruit = get_dict_bruit(list_fichier)\n",
    "dict_bruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#Segmentation en token liste de liste\n",
    "#Mettre les caractères en minuscule\n",
    "def tokenization (list_spk,dict_bruit):\n",
    "    list_token = []\n",
    "    for key, value in dict_bruit.items():\n",
    "        for i in list_spk:\n",
    "            h = i[2]\n",
    "            #i[2]=h.lower()\n",
    "            i[2] = re.sub(value,key, i[2])\n",
    "            list_token.append(i[2].split(\" \"))\n",
    "    return list_token   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_tuple (tuple_spk):\n",
    "    list_spk = []\n",
    "    for i in tuple_spk:\n",
    "        x = [i[0], i[1], i[2]]\n",
    "        list_spk.append(x)\n",
    "    return list_spk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segmentation en token liste de liste\n",
    "#Mettre les caractères en minuscule\n",
    "def tokenization (list_spk):\n",
    "    list_token = []\n",
    "    for i in list_spk:\n",
    "        h = i[2]\n",
    "        i[2]=h.lower()\n",
    "        i[2] = re.sub(r\"\\[(.*?)\\]\",\"#bruit\",i[2])\n",
    "        i[2] = re.sub(r\"\\{(.*?)\\}\",\"#bruit\",i[2])\n",
    "        i[2] = re.sub(r\"el-\",\"el\",i[2])\n",
    "        list_token.append(re.split(\"\\s|-\", i[2]))\n",
    "        #ou\n",
    "        #list_token.append(i[2].split(\" \"))\n",
    "    return list_token   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path_LeFFF, encoding = 'utf-8')\n",
    "list_fr = []\n",
    "for i in f:\n",
    "    list_fr.append(i.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Détection de l'alternance codique (pensez à mettre des exceptions pour les tokens qui existent dans les deux langues).\n",
    "def detect_fr (list_token):\n",
    "    list_tokenfr=[]\n",
    "    for j in list_token:\n",
    "        list_temp= []\n",
    "        for i in range(0, len(j)):\n",
    "            if j[i].startswith(\"#\"):\n",
    "                list_temp.append(j[i])\n",
    "            elif j[i] == 'b':\n",
    "                list_temp.append(j[i])\n",
    "            elif j[i] == 'bi':\n",
    "                list_temp.append(j[i])\n",
    "            elif j[i] == 'fi':\n",
    "                list_temp.append(j[i])\n",
    "            elif j[i] == 'fil':\n",
    "                list_temp.append(j[i])\n",
    "            elif j[i] == 'inti':\n",
    "                list_temp.append(j[i])\n",
    "            elif j[i] == 'ma':\n",
    "                list_temp.append(j[i])\n",
    "            elif j[i] == 'haka':\n",
    "                list_temp.append(j[i])\n",
    "            elif j[i] == 'li':\n",
    "                list_temp.append(j[i]) \n",
    "            elif j[i] == 'ka':\n",
    "                list_temp.append(j[i])\n",
    "            elif j[i] == 'w':\n",
    "                list_temp.append(j[i])\n",
    "            elif j[i] == 'ya':\n",
    "                list_temp.append(j[i])\n",
    "            elif j[i].startswith(\"\\'\"):\n",
    "                list_temp.append(j[i])\n",
    "            elif \"\\'\" in j[i]:\n",
    "                x = j[i].split(\"\\'\")\n",
    "                #x[0] = x[0]+\"\\'\"\n",
    "                for nbsplit in range(0, len(x)):\n",
    "                    x[nbsplit] = '#'+x[nbsplit]\n",
    "                    list_temp.append(x[nbsplit])\n",
    "            elif j[i] in list_fr:\n",
    "                j[i] = '#'+j[i]\n",
    "                #print(j[i])\n",
    "                list_temp.append(j[i])\n",
    "            else:\n",
    "                list_temp.append(j[i])\n",
    "            #print(j[i])\n",
    "        list_tokenfr.append(list_temp)\n",
    "    return list_tokenfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Remplacement de caractères spéciaux FBB\n",
    "def regex_FBB (list_spk):\n",
    "    list_spk_mod=[]\n",
    "    for i in (list_spk):\n",
    "        list_temp=[]\n",
    "        for lignes in i:\n",
    "            lignes = lignes.split(\" \")\n",
    "            for ligne in range(0, len(lignes)):\n",
    "                if lignes[ligne].startswith('#'):\n",
    "                    #print(lignes[ligne])\n",
    "                    pass\n",
    "                else:\n",
    "                    lignes[ligne] = re.sub(r\"ž\",\"j\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ḥ\",\"h\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"š\",\"ch\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ԑ\",\"3\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ġ\",\"gh\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"đ\",\"th\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ď\",\"th\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ŧ\",\"th\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"e|é|è|ê\",\"e\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"\\-\",\"\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ç\",\"s\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"x|×\",\"kh\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ù|ü|û\",\"u\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"Ö|ö|Ô|ô|Ò|ò|Õ|õ|Ó|ó\",\"o\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"à|â|ä\",\"a\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ï|î\",\"i\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"\\'\",\"\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"à\",\"a\",lignes[ligne])\n",
    "                list_temp.append(' '.join(lignes))\n",
    "        list_spk_mod.append(list_temp)\n",
    "    return list_spk_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remplacement de caractères spéciaux YBA\n",
    "def regex_YBA (list_spk):\n",
    "    list_spk_mod=[]\n",
    "    for i in (list_spk):\n",
    "        list_temp=[]\n",
    "        for lignes in i:\n",
    "            lignes = lignes.split(\" \")\n",
    "            for ligne in range(0, len(lignes)):\n",
    "                if lignes[ligne].startswith('#'):\n",
    "                    #print(lignes[ligne])\n",
    "                    pass\n",
    "                else:\n",
    "                    lignes[ligne] = re.sub(r\"ṛ\",\"r\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ž\",\"j\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ḥ\",\"h\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"š\",\"ch\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ԑ\",\"3\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ġ\",\"gh\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ṭ\",\"t\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"đ|ḍ\",\"th\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ď\",\"th\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ŧ\",\"th\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"e|é|è|ê\",\"e\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"\\-\",\"\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ç|ṣ\",\"s\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"x|×\",\"kh\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ù|ü|û\",\"u\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"Ö|ö|Ô|ô|Ò|ò|Õ|õ|Ó|ó\",\"o\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"à|â|ä\",\"a\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"ï|î\",\"i\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"\\'\",\"\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"\\'\",\"\",lignes[ligne])\n",
    "                    lignes[ligne] = re.sub(r\"à\",\"a\",lignes[ligne])\n",
    "                list_temp.append(' '.join(lignes))\n",
    "        list_spk_mod.append(list_temp)\n",
    "    return list_spk_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic = 'ar-t-i0-und'\n",
    "\n",
    "import http.client\n",
    "import json\n",
    "\n",
    "def request(input, itc):\n",
    "    conn = http.client.HTTPSConnection('inputtools.google.com')\n",
    "    conn.request('GET', '/request?text=' + input + '&itc=' + itc + '&num=1&cp=0&cs=1&ie=utf-8&oe=utf-8&app=test')\n",
    "    res = conn.getresponse()\n",
    "    return res\n",
    "\n",
    "def driver(input, itc):\n",
    "    output = ''\n",
    "    if ' ' in input:\n",
    "        input = input.split(' ')\n",
    "        for i in input:\n",
    "            res = request(input = i, itc = itc)\n",
    "            res = res.read()\n",
    "            if i==0:\n",
    "                output = str(res, encoding = 'utf-8')[14+4+len(i):-31]\n",
    "            else:\n",
    "                output = output + ' ' + str(res, encoding = 'utf-8')[14+4+len(i):-31]\n",
    "    else:\n",
    "        res = request(input = input, itc = itc)\n",
    "        res = res.read()\n",
    "        output = str(res, encoding = 'utf-8')[14+4+len(input):-31]\n",
    "    return output\n",
    "    \n",
    "def translitteration (list_spk_mod):\n",
    "    list_spk_trans = []\n",
    "    for ligne in list_spk_mod:\n",
    "        list_temp = []\n",
    "        #print(ligne)\n",
    "        for mot in ligne:\n",
    "            if mot.startswith('#'):\n",
    "                list_temp.append(mot.lstrip('#'))\n",
    "            else:\n",
    "                x = driver(mot, arabic)\n",
    "                list_temp.append(str(x))\n",
    "        list_spk_trans.append(list_temp)\n",
    "        #print(list_temp)\n",
    "\n",
    "    return list_spk_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delBruit(list_spk_trans):\n",
    "    for phrase in list_spk_trans:\n",
    "        while \"#bruit\" in phrase:\n",
    "            phrase.remove('#bruit')\n",
    "    return list_spk_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-19 16:52:37,947 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2021-11-19 16:52:43,683 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2021-11-19 16:53:11,667 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "segmenter = FarasaSegmenter(interactive=True)\n",
    "parser = tagger(interactive=True)\n",
    "stemmer = FarasaStemmer(interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg(list_spk_trans):\n",
    "    list_spk_seg = []\n",
    "    for phrase in list_spk_trans:\n",
    "        seg = segmenter.segment(' '.join(phrase))\n",
    "        list_spk_seg.append(seg)\n",
    "    return list_spk_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POStag(list_spk_trans):\n",
    "    list_spk_parsed = []\n",
    "    for phrase in list_spk_trans:\n",
    "        pos_tagged = parser.tag(' '.join(phrase))\n",
    "        list_spk_parsed.append(pos_tagged)\n",
    "    return list_spk_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POStag(list_spk_trans):\n",
    "    list_spk_parsed = []\n",
    "    for phrase in list_spk_trans:\n",
    "        pos_tagged = parser.tag(' '.join(phrase))\n",
    "        list_spk_parsed.append(pos_tagged)\n",
    "    return list_spk_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(list_spk_trans):\n",
    "    list_spk_stemmed = []\n",
    "    for phrase in list_spk_trans:\n",
    "        lemmes = stemmer.stem(' '.join(phrase))\n",
    "        list_spk_stemmed.append(lemmes)\n",
    "    return list_spk_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_content2(list_spk_parsed, list_spk):\n",
    "    list_POS = []\n",
    "    list_token = []\n",
    "    list_text = []\n",
    "\n",
    "    for ligne, text in zip(list_spk_parsed, list_spk):\n",
    "        #list_text_temp = []\n",
    "        list_POS_temp = []\n",
    "        list_token_temp = []\n",
    "        list_token_fr_temp = []\n",
    "\n",
    "        ligne = re.sub(\"S/S\",\"\",ligne)\n",
    "        ligne = re.sub(\"E/E\",\"\",ligne)\n",
    "        ligne = ''.join(ligne)\n",
    "        ligne = re.sub(r\"\\s\\+\",\"+\",ligne)\n",
    "        ligne = re.sub(r\"\\+\\s\",\"+\",ligne)\n",
    "        ligne = re.sub(r\"\\s\\\\s\",\"+\",ligne)\n",
    "\n",
    "        #remplacer les faux départs xxx() par XXX~, la pos par X  \n",
    "        ligne = re.sub(r\" \\(/PUNC \\)/PUNC\",\"؛\",ligne)\n",
    "        l = ligne.split(\" \")\n",
    "        \n",
    "        \n",
    "        for i in list_spk:\n",
    "            h = i[2]\n",
    "            i[2]=h.lower()\n",
    "            i[2] = re.sub(r\"\\[(.*?)\\]\",\"#bruit\",i[2])\n",
    "            i[2] = re.sub(r\"el-\",\"el\",i[2])\n",
    "            list_text.append(re.split(\"\\s|-|'\", i[2]))\n",
    "        '''\n",
    "        for i in list_text:\n",
    "            for j in i:\n",
    "                while '' in j:\n",
    "                    i.remove(j)\n",
    "        '''\n",
    "        while '' in l:\n",
    "            l.remove('')\n",
    "        while '' in list_text:\n",
    "            list_text.remove('')\n",
    "\n",
    "        for i in l:\n",
    "            POS = re.sub(r\"[\\u0600-\\u06FF\\/a-z\\?]+\",\"\",i)\n",
    "            TOKEN = re.sub(r\"[A-Z\\-\\/a-z]+\",\"\",i)\n",
    "            TOKEN_fr = re.sub(r\"[\\u0600-\\u06FF\\/A-Z\\?\\-\\+]+\",\"\",i)\n",
    "            POS = POS.rstrip(\"+\")\n",
    "            TOKEN = TOKEN.rstrip(\"+\")\n",
    "            POS = POS.lstrip(\"+\")\n",
    "            TOKEN = TOKEN.lstrip(\"+\")\n",
    "            \n",
    "            list_POS_temp.append(POS)\n",
    "            list_token_temp.append(TOKEN)\n",
    "            list_token_fr_temp.append(TOKEN_fr)\n",
    "\n",
    "        if len(list_POS_temp) != len(list_token_temp):\n",
    "            print('error')\n",
    "\n",
    "        for fr, ar in zip(list_token_fr_temp, list_token_temp):\n",
    "            if fr != '':\n",
    "                list_token_temp[list_token_temp.index(ar)] = fr\n",
    "\n",
    "        list_POS.append(list_POS_temp)\n",
    "        list_token.append(list_token_temp)\n",
    "\n",
    "    return list_token, list_POS, list_text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_foreign(list_token, list_POS):\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "    for list_tokens, list_poss in zip(list_token, list_POS):\n",
    "        for token, pos in zip(list_tokens, list_poss):\n",
    "            if token == 'l' or token == 'd' or token == 'c' or token == 's' or token == 'qu' or token == 'm' or token == 't' or token == 'n':\n",
    "                token_annot = token+\"\\'\"\n",
    "                token = token+\"e\"\n",
    "\n",
    "            if 'FOREIGN' in pos:\n",
    "\n",
    "                doc = nlp(token)\n",
    "\n",
    "                for mot in doc:\n",
    "                    pos_ = mot.pos_\n",
    "                    '''\n",
    "                    print(mot.text) \n",
    "                    print(mot.lemma_)\n",
    "                    print(mot.pos_)\n",
    "                    print(' ')\n",
    "                    '''\n",
    "                list_poss[list_poss.index(pos)] = pos_\n",
    "        list_POS[list_POS.index(list_poss)] = list_poss\n",
    "    \n",
    "    return list_token, list_POS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToTuple (list_token):\n",
    "    # reconversion sous la forme de liste de tuple\n",
    "    content = []\n",
    "    for i in list_token:\n",
    "        tuple_ = (i[0], i[1], i[2])\n",
    "        content.append(tuple_)\n",
    "    return content\n",
    "\n",
    "def make_content(list_token, list_POS, list_lemma, list_spk, list_text):\n",
    "    POS_content = []\n",
    "    token_content = []\n",
    "    lemma_content = []\n",
    "    text_content = []\n",
    "    \n",
    "    for token, pos, lemma, times, text in zip(list_token, list_POS, list_lemma, list_spk, list_text):\n",
    "        t = ' '.join([str(unit) for unit in token])\n",
    "        token_content.append([int(times[0]), int(times[1]), t])\n",
    "        \n",
    "        p = ' '.join([str(unit) for unit in pos])\n",
    "        POS_content.append([int(times[0]), int(times[1]), p])\n",
    "        \n",
    "        lemma_content.append([int(times[0]), int(times[1]), lemma])\n",
    "        \n",
    "        text_content.append([int(times[0]), int(times[1]), text])\n",
    "    \n",
    "    for i in list_spk:\n",
    "        if i[2]=='':\n",
    "            list_spk.remove(list_spk[list_spk.index(i)])\n",
    "            \n",
    "    for i, j, k, l in zip(token_content, POS_content, lemma_content, text_content):\n",
    "        if i[0]>i[1]:\n",
    "            end = i[0]\n",
    "            start = i[1]\n",
    "            token_content[token_content.index(i)][0] = start\n",
    "            token_content[token_content.index(i)][1] = end\n",
    "        if j[0]>j[1]:\n",
    "            end = j[0]\n",
    "            start = j[1]\n",
    "            POS_content[POS_content.index(j)][0] = start\n",
    "            POS_content[POS_content.index(j)][1] = end\n",
    "        if k[0]>k[1]:\n",
    "            end = k[0]\n",
    "            start = k[1]\n",
    "            lemma_content[lemma_content.index(k)][0] = start\n",
    "            lemma_content[lemma_content.index(k)][1] = end\n",
    "        if l[0]>l[1]:\n",
    "            end = l[0]\n",
    "            start = l[1]\n",
    "            text_content[text_content.index(l)][0] = start\n",
    "            text_content[text_content.index(l)][1] = end        \n",
    "        \n",
    "        if i[0]==i[1] and i[2]=='':\n",
    "            token_content.remove(token_content[token_content.index(i)])\n",
    "        if i[0]==i[1]:\n",
    "            token_content.remove(token_content[token_content.index(i)])\n",
    "            \n",
    "        if j[0]==j[1] and j[2]=='':\n",
    "            POS_content.remove(POS_content[POS_content.index(j)])\n",
    "        if j[0]==j[1]:\n",
    "            POS_content.remove(POS_content[POS_content.index(j)])\n",
    "            \n",
    "        if k[0]==k[1] and k[2]=='':\n",
    "            lemma_content.remove(lemma_content[lemma_content.index(k)])\n",
    "        if k[0]==k[1]:\n",
    "            lemma_content.remove(lemma_content[lemma_content.index(k)])\n",
    "            \n",
    "        if l[0]==l[1] and l[2]=='':\n",
    "            text_content.remove(text_content[text_content.index(l)])\n",
    "        if l[0]==l[1]:\n",
    "            text_content.remove(text_content[text_content.index(l)])\n",
    "        \n",
    "        if i[2]=='':\n",
    "            token_content.remove(token_content[token_content.index(i)])\n",
    "        if j[2]=='':\n",
    "            POS_content.remove(POS_content[POS_content.index(j)])\n",
    "        if k[2]=='':\n",
    "            lemma_content.remove(lemma_content[lemma_content.index(k)])\n",
    "        if l[2]=='':\n",
    "            text_content.remove(text_content[text_content.index(l)])\n",
    "            \n",
    "    '''\n",
    "    for i, j, k, l in zip(token_content, POS_content, lemma_content, text_content):\n",
    "        if i[2]=='':\n",
    "            token_content[token_content.index(i)][2] = 'vide'\n",
    "        if j[2]=='':\n",
    "            POS_content[POS_content.index(j)][2] = 'vide'\n",
    "        if k[2]=='':\n",
    "            lemma_content[lemma_content.index(k)][2] = 'vide'\n",
    "        if l[2]=='':\n",
    "            text_content[text_content.index(l)][2] = 'vide'\n",
    "    '''\n",
    "    for i in text_content:\n",
    "        i[2] = ' '.join(i[2])\n",
    "    \n",
    "    token_content = listToTuple(token_content)\n",
    "    POS_content = listToTuple(POS_content)\n",
    "    lemma_content = listToTuple(lemma_content)\n",
    "    text_content = listToTuple(text_content)\n",
    "    \n",
    "    return token_content, POS_content, lemma_content, text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_error(text_content, token_content, POS_content, list_spk_parsed):\n",
    "    error_content = []\n",
    "    \n",
    "    for i, j, h, k in zip(text_content, token_content, POS_content, list_spk_parsed):\n",
    "        list_error_temp = []\n",
    "        \n",
    "        if len(i[2].split(' ')) != len(h[2].split(' ')):\n",
    "            if len(i[2].split(' ')) > len(j[2].split(' ')):\n",
    "                list_error_temp.append(i[0])\n",
    "                list_error_temp.append(i[1])\n",
    "                list_error_temp.append('error latin')\n",
    "                error_content.append(list_error_temp)\n",
    "            elif len(i[2].split(' ')) < len(j[2].split(' ')):\n",
    "                list_error_temp.append(i[0])\n",
    "                list_error_temp.append(i[1])\n",
    "                list_error_temp.append('error transliteration')\n",
    "                error_content.append(list_error_temp)   \n",
    "        '''\n",
    "        else:\n",
    "            list_error_temp.append(i[0])\n",
    "            list_error_temp.append(i[1])\n",
    "            list_error_temp.append('')\n",
    "        '''\n",
    "        token_content = listToTuple(token_content)\n",
    "    return error_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sent_id(text_content):\n",
    "    list_sent_id = []\n",
    "    count = 0\n",
    "    for i in text_content:\n",
    "        if i[2]!='':\n",
    "            list_temp = []\n",
    "            count+=1\n",
    "            sent_id = fichier.rstrip('A.eaf')+tier_name+'_'+str(count)\n",
    "            list_temp.append(i[0])\n",
    "            list_temp.append(i[1])\n",
    "            list_temp.append(sent_id)\n",
    "            list_sent_id.append(list_temp)\n",
    "\n",
    "    sent_id_content = listToTuple(list_sent_id)\n",
    "    return sent_id_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation(tierName, content):\n",
    "    eafob.add_tier(tierName)\n",
    "    for i in content:\n",
    "        t1 = int(i[0])\n",
    "        t2 = int(i[1])\n",
    "        value = str(i[2])\n",
    "        eafob.add_annotation(tierName, t1, t2, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for fichier in list_fichier:\n",
    "    print('')\n",
    "    print(fichier)\n",
    "    fichier_in = path_in + fichier\n",
    "    eafob = pympi.Elan.Eaf(fichier_in)\n",
    "\n",
    "    tier_names = eafob.get_tier_names()\n",
    "    print(list(tier_names))\n",
    "    \n",
    "    for tier_name in list(tier_names):\n",
    "        if tier_name.startswith('#')== False:\n",
    "            spk = eafob.get_annotation_data_for_tier(tier_name)\n",
    "\n",
    "            list_spk = list_tuple(spk)\n",
    "\n",
    "            list_spk_token = tokenization(list_spk)\n",
    "\n",
    "            #list_trans_fr&ar\n",
    "\n",
    "            list_spk_token_fr = detect_fr(list_spk_token)\n",
    "            print(tier_name, ' ok: detect_fr')\n",
    "\n",
    "            #list_spk_mod = regex_FBB(list_spk_token_fr)\n",
    "            \n",
    "            list_spk_mod = regex_YBA(list_spk_token_fr)\n",
    "            print(tier_name, ' ok: regex')\n",
    "            \n",
    "            list_spk_trans = translitteration(list_spk_mod)\n",
    "            print(tier_name, ' ok: transliteration')\n",
    "\n",
    "            #list_spk_trans = delBruit(list_spk_trans)\n",
    "\n",
    "            list_spk_parsed = POStag(list_spk_trans)\n",
    "            print(tier_name, ' ok: parser')\n",
    "\n",
    "            list_spk_stemmed = lemmatization(list_spk_trans)\n",
    "            print(tier_name, ' ok: stemmer')\n",
    "\n",
    "            #list_token, list_POS = edit_content(list_spk_parsed, list_spk_seg)\n",
    "\n",
    "            list_token, list_POS, list_text = edit_content2(list_spk_parsed, list_spk)\n",
    "            print(tier_name, ' ok: edit_content')\n",
    "\n",
    "            list_token, list_POS = parse_foreign(list_token, list_POS)\n",
    "            print(tier_name, ' ok: spacy')\n",
    "\n",
    "            token_content, POS_content, lemma_content, text_content = make_content(list_token, list_POS, list_spk_stemmed, list_spk, list_text)\n",
    "            print(tier_name, ' ok: make_content')\n",
    "\n",
    "            error_content = detect_error(text_content, token_content, POS_content, list_spk_parsed)\n",
    "            \n",
    "            sent_id_content = make_sent_id(text_content)\n",
    "\n",
    "            list_ = []\n",
    "            annotation(tier_name+\"_sent_id\", sent_id_content)\n",
    "            annotation(tier_name+\"_trans\", token_content)\n",
    "            annotation(tier_name+\"_trans_seg\", list_)\n",
    "            annotation(tier_name+\"_trans_seg_eval\", list_)\n",
    "            annotation(tier_name+\"_POS\", POS_content)\n",
    "            annotation(tier_name+\"_POS_seg\", list_)\n",
    "            annotation(tier_name+\"_POS_seg_eval\", list_)\n",
    "            annotation(tier_name+\"_lemma\", lemma_content)\n",
    "            annotation(tier_name+\"_word_based\", text_content)       \n",
    "            annotation(tier_name+\"_word_based_seg\", list_)\n",
    "            annotation(tier_name+\"_errors\", error_content)\n",
    "        \n",
    "    fichier = fichier.rstrip(\".eaf\")\n",
    "    fichier_out = path_out+fichier\n",
    "    fichier_out = fichier_out.rstrip(\".eaf\")\n",
    "    print(fichier_out)\n",
    "    \n",
    "    pympi.Elan.to_eaf(fichier_out+\"_annot.eaf\", eafob, pretty=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
